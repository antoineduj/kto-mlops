{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create s3 client in order to download and upload data from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    endpoint_url=\"http://minio-api-blabla-dev.apps.sandbox-m3.666.p1.openshiftapps.com\",\n",
    "    aws_access_key_id=\"minio\",\n",
    "    aws_secret_access_key=\"minio123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract labels from annotations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "def extraction_from_annotation_file(bucket_name: str, s3_path: str, filename: str, s3_client) -> tuple[dict[Any, Any], set[Any]]:\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    s3_client.download_file(bucket_name, s3_path, filename)\n",
    "\n",
    "    extract = {}\n",
    "    classes = set()\n",
    "    with open(filename) as file:\n",
    "        annotations = json.load(file)[\"annotations\"]\n",
    "        for annotation in annotations:\n",
    "            label = annotation[\"annotation\"][\"label\"]\n",
    "            extract[annotation[\"fileName\"]] = label\n",
    "            classes.add(label)\n",
    "    return extract, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "working_dir = \"./dist\"\n",
    "bucket_name = \"cats-dogs-other\"\n",
    "extract, classes = extraction_from_annotation_file(bucket_name, \n",
    "                                                    \"dataset/cats_dogs_others-annotations.json\",\n",
    "                                                    working_dir + \"/cats_dogs_others-annotations.json\",\n",
    "                                                    s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random split train / evaluate / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = working_dir + \"/train\"\n",
    "evaluate_dir = working_dir + \"/evaluate\"\n",
    "test_dir = working_dir + \"/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def random_split_train_evaluate_test_from_extraction(extract: dict,\n",
    "                                                     classes: set,\n",
    "                                                     split_ratio_train: float,\n",
    "                                                     split_ratio_evaluate: float,\n",
    "                                                     split_ratio_test: float,\n",
    "                                                     train_dir: str,\n",
    "                                                     evaluate_dir: str,\n",
    "                                                     test_dir: str,\n",
    "                                                     bucket_name: str,\n",
    "                                                     s3_path: str,\n",
    "                                                     s3_client):\n",
    "\n",
    "    if split_ratio_train + split_ratio_evaluate + split_ratio_test != 1:\n",
    "        raise Exception(\"sum of ratio must be equal to 1\")\n",
    "\n",
    "    keys_list = list(extract.keys())  # shuffle() wants a list\n",
    "    random.shuffle(keys_list)  # randomize the order of the keys\n",
    "\n",
    "    nkeys_train = int(split_ratio_train * len(keys_list))  # how many keys does split ratio train% equal\n",
    "    keys_train = keys_list[:nkeys_train]\n",
    "    keys_evaluate_and_test = keys_list[nkeys_train:]\n",
    "\n",
    "    split_ratio_evaluate_and_test = split_ratio_evaluate + split_ratio_test\n",
    "    nkeys_evaluate = int((split_ratio_evaluate / split_ratio_evaluate_and_test) * len(keys_evaluate_and_test))\n",
    "    keys_evaluate = keys_evaluate_and_test[:nkeys_evaluate]\n",
    "    keys_test = keys_evaluate_and_test[nkeys_evaluate:]\n",
    "\n",
    "    extract_train = {k: extract[k] for k in keys_train}\n",
    "    extract_evaluate = {k: extract[k] for k in keys_evaluate}\n",
    "    extract_test = {k: extract[k] for k in keys_test}\n",
    "\n",
    "    # create directories\n",
    "    for existing_class in classes:\n",
    "        Path(train_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "        Path(evaluate_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "        Path(test_dir + \"/\" + existing_class).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add files in directories\n",
    "    download_files(extract_train, train_dir, bucket_name, s3_path, s3_client)\n",
    "    download_files(extract_evaluate, evaluate_dir, bucket_name, s3_path, s3_client)\n",
    "    download_files(extract_test, test_dir, bucket_name, s3_path, s3_client)\n",
    "\n",
    "\n",
    "def download_files(extract: dict, directory: str, bucket_name: str, s3_path: str, s3_client):\n",
    "    for key, value in extract.items():\n",
    "        s3_client.download_file(bucket_name, s3_path + key, directory + \"/\" + value + \"/\" + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "split_ratio_train = 0.8\n",
    "split_ratio_evaluate = 0.1\n",
    "split_ratio_test = 0.1\n",
    "\n",
    "random_split_train_evaluate_test_from_extraction(extract, classes, split_ratio_train,\n",
    "                                                 split_ratio_evaluate, split_ratio_test,\n",
    "                                                 train_dir, evaluate_dir, test_dir, bucket_name,\n",
    "                                                 \"dataset/extract/\", s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & evaluate ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_filename = \"final_model.keras\"\n",
    "model_plot_filename = \"model_plot.png\"\n",
    "batch_size = 64 \n",
    "epochs = 4\n",
    "\n",
    "# train & evaluate\n",
    "model_dir = working_dir + \"/model\"\n",
    "model_path = model_dir + \"/\" + model_filename\n",
    "plot_filepath = model_dir + \"/\" + model_plot_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from keras import Model\n",
    "from keras.src.applications.vgg16 import VGG16\n",
    "from keras.src.callbacks import History\n",
    "from keras.src.layers import Dropout, Flatten, Dense\n",
    "from keras.src.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def train_and_evaluate_model(train_dir: str,\n",
    "                             evaluate_dir: str,\n",
    "                             test_dir: str,\n",
    "                             model_dir: str,\n",
    "                             model_path: str,\n",
    "                             plot_filepath: str,\n",
    "                             batch_size: int,\n",
    "                             epochs: int):\n",
    "    model = define_model()\n",
    "\n",
    "    # create data generator\n",
    "    datagen = ImageDataGenerator(featurewise_center=True)\n",
    "    # specify imagenet mean values for centering\n",
    "    datagen.mean = [123.68, 116.779, 103.939]\n",
    "    # prepare iterator\n",
    "    train_it = datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    validation_it = datagen.flow_from_directory(\n",
    "        evaluate_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    # fit model\n",
    "    history = model.fit(\n",
    "        train_it,\n",
    "        steps_per_epoch=len(train_it),\n",
    "        validation_data=validation_it,\n",
    "        validation_steps=len(validation_it),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "    )\n",
    "    # test model\n",
    "    evaluate_it = datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        class_mode=\"binary\",\n",
    "        batch_size=batch_size,\n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    _, acc = model.evaluate(evaluate_it, steps=len(evaluate_it), verbose=1)\n",
    "    evaluate_accuracy_percentage = acc * 100.0\n",
    "    print(\"> %.3f\" % evaluate_accuracy_percentage)\n",
    "\n",
    "    Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    create_history_plots(history, plot_filepath)\n",
    "\n",
    "    model.save(model_path)\n",
    "\n",
    "def define_model() -> Model:\n",
    "    model = VGG16(include_top=False, input_shape=(224, 224, 3))\n",
    "    # mark loaded layers as not trainable\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # add new classifier layers\n",
    "    output = model.layers[-1].output\n",
    "    drop1 = Dropout(0.2)(output)\n",
    "    flat1 = Flatten()(drop1)\n",
    "    class1 = Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\")(flat1)\n",
    "    output = Dense(3, activation=\"sigmoid\")(class1)\n",
    "    # define new model\n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_history_plots(history: History, plot_filepath: str):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title(\"Cross Entropy Loss\")\n",
    "    pyplot.plot(history.history[\"loss\"], color=\"blue\", label=\"train\")\n",
    "    pyplot.plot(history.history[\"val_loss\"], color=\"orange\", label=\"test\")\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title(\"Classification Accuracy\")\n",
    "    pyplot.plot(history.history[\"accuracy\"], color=\"blue\", label=\"train\")\n",
    "    pyplot.plot(history.history[\"val_accuracy\"], color=\"orange\", label=\"test\")\n",
    "    # save plot to file\n",
    "    pyplot.savefig(plot_filepath)\n",
    "    pyplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_and_evaluate_model(train_dir, evaluate_dir, test_dir, model_dir, model_path,\n",
    "                         plot_filepath, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# load and prepare the image\n",
    "def load_image(filename: str|BytesIO):\n",
    "    # load the image\n",
    "    img = load_img(filename, target_size=(224, 224))\n",
    "    # convert to array\n",
    "    img = img_to_array(img)\n",
    "    # reshape into a single sample with 3 channels\n",
    "    img = img.reshape(1, 224, 224, 3)\n",
    "    # center pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img - [123.68, 116.779, 103.939]\n",
    "    return img\n",
    "\n",
    "class Inference:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = load_model(model_path)\n",
    "\n",
    "    def execute(self, filepath:str|BytesIO):\n",
    "        img = load_image(filepath)\n",
    "        result = self.model.predict(img)\n",
    "        values = [float(result[0][0]), float(result[0][1]), float(result[0][2])]\n",
    "        switcher = ['Cat', 'Dog', 'Other']\n",
    "        prediction = np.argmax(result[0])\n",
    "        return {\"prediction\": switcher[prediction], \"values\": values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def test_model(model_inference: Inference, model_dir: str, test_dir: str):\n",
    "    statistics = {\"ok\": 0, \"ko\": 0, \"total\": 0}\n",
    "    results = []\n",
    "    path_test_dir = Path(test_dir)\n",
    "    for path in path_test_dir.glob(\"**/*\"):\n",
    "        if path.is_dir():\n",
    "            continue\n",
    "        model_result = model_inference.execute(str(path))\n",
    "\n",
    "        prediction = model_result[\"prediction\"]\n",
    "        prediction_truth = path.parent.name.lower().replace(\"s\", \"\")\n",
    "        status = prediction_truth == prediction.lower()\n",
    "        statistics[\"ok\" if status else \"ko\"] += 1\n",
    "        result = {\n",
    "            \"filename\": path.name,\n",
    "            \"ok\": status,\n",
    "            \"prediction\": prediction,\n",
    "            \"prediction_truth\": prediction_truth,\n",
    "            \"values\": model_result[\"values\"],\n",
    "        }\n",
    "        results.append(result)\n",
    "    statistics[\"total\"] = statistics[\"ok\"] + statistics[\"ko\"]\n",
    "\n",
    "    with open(model_dir + \"/statistics.json\", \"w\") as file_stream:\n",
    "        json.dump(statistics, file_stream, indent=4)\n",
    "\n",
    "    with open(model_dir + \"/predictions.json\", \"w\") as file_stream:\n",
    "        json.dump(results, file_stream, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
